{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvi/8E46XNqOy7zUxTIxBU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/encoras/Artificial-Intelligence-Group/blob/master/Sentiment_analysis_IMDB_movie_review_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from mpl_toolkits import mplot3d\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "from scipy.cluster import hierarchy\n",
        "import seaborn as sns\n",
        "from sklearn import svm\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
        "from sklearn.datasets import load_iris, load_wine, fetch_20newsgroups, fetch_openml\n",
        "from sklearn.impute import MissingIndicator, SimpleImputer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier,\n",
        "    RandomForestRegressor,\n",
        "    GradientBoostingRegressor,\n",
        "    AdaBoostRegressor,\n",
        "    GradientBoostingClassifier,\n",
        "    AdaBoostClassifier\n",
        ")\n",
        "from sklearn.feature_extraction.text import (\n",
        "    CountVectorizer,\n",
        "    TfidfTransformer,\n",
        "    TfidfVectorizer\n",
        ")\n",
        "from sklearn.linear_model import (\n",
        "    LinearRegression,\n",
        "    LogisticRegression,\n",
        "    LogisticRegressionCV,\n",
        "    Lasso,\n",
        "    Ridge,\n",
        "    ElasticNet\n",
        ")\n",
        "from sklearn.metrics import (\n",
        "    mean_absolute_error,\n",
        "    mean_squared_error,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    ConfusionMatrixDisplay,\n",
        "    accuracy_score\n",
        ")\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split,\n",
        "    GridSearchCV,\n",
        "    cross_val_score,\n",
        "    cross_validate\n",
        ")\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.preprocessing import (\n",
        "    MinMaxScaler,\n",
        "    StandardScaler,\n",
        "    OrdinalEncoder,\n",
        "    LabelEncoder,\n",
        "    OneHotEncoder,\n",
        "    PolynomialFeatures\n",
        ")\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.multiclass import OneVsRestClassifier"
      ],
      "metadata": {
        "id": "G4uBzrAx3jnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets hdbscan keybert"
      ],
      "metadata": {
        "id": "9slBz1sG8v0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract keywords from texts\n",
        "# used to assign meaningful names to clusters\n",
        "from keybert import KeyBERT"
      ],
      "metadata": {
        "id": "kfvEVrVEVGoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading IMDB data set**\n",
        "\n",
        "### About Dataset\n",
        "IMDB dataset having 50K movie reviews for natural language processing or Text analytics.\n",
        "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training and 25,000 for testing. So, predict the number of positive and negative reviews using either classification or deep learning algorithms.\n",
        "For more dataset information, please go through the following link,\n",
        "http://ai.stanford.edu/~amaas/data/sentiment/\n",
        "\n",
        "examples of data analysis:\n",
        "\n",
        "https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
      ],
      "metadata": {
        "id": "kc8b120fSFj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset_train = load_dataset('imdb',split='train')\n",
        "dataset_train.rename_column('label', 'labels')\n",
        "train_df=pd.DataFrame(dataset_train)\n",
        "\n",
        "dataset_test = load_dataset('imdb',split='test')\n",
        "dataset_test.rename_column('label', 'labels')\n",
        "test_df=pd.DataFrame(dataset_test)"
      ],
      "metadata": {
        "id": "wM2Y7ymf6yXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train\n",
        "train_df"
      ],
      "metadata": {
        "id": "zAejLSsc8_m9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['text'][0]"
      ],
      "metadata": {
        "id": "yD8NwE3MObLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Remove HTML code tags**"
      ],
      "metadata": {
        "id": "s7jVZCc5ZnYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['text'] = train_df['text'].replace(r'<[^<]+?>', ' ', regex=True)"
      ],
      "metadata": {
        "id": "SgrOR6zBZW7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['text'][0]"
      ],
      "metadata": {
        "id": "e1eJ0wGbZ0a1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remove numbers Covid19, 20th, 1965s**"
      ],
      "metadata": {
        "id": "99JxbtgTafAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['text'] = train_df['text'].str.replace(r'\\b\\w*\\d\\w*\\b', '', regex=True)\n",
        "# Optional: clean up extra spaces afterward\n",
        "train_df['text'] = train_df['text'].str.replace(r'\\s+', ' ', regex=True).str.strip()"
      ],
      "metadata": {
        "id": "M3suCo3eanp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['text'][0]"
      ],
      "metadata": {
        "id": "AlKj7L-6bE6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the 5 most common junk words completely from the text\n",
        "junk_words = ['café', 'hindus', 'shirdi', 'sai', 'baba', 'cain', 'abel', 'sodom', 'gomorrah', 'la', 'mj', 'california']\n",
        "\n",
        "import re\n",
        "for word in junk_words:\n",
        "    train_df['text'] = train_df['text'].str.replace(rf'\\b{word}\\b', ' ', regex=True, case=False)\n",
        "    test_df['text']  = test_df['text'].str.replace(rf'\\b{word}\\b', ' ', regex=True, case=False)"
      ],
      "metadata": {
        "id": "PFlv5O6GPJvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://mpolinowski.github.io/docs/Development/Python/2023-05-20-python-sklearn-cheat-sheet/2023-05-20/\n",
        "train_df.info()\n",
        "train_df['text'].str.isspace().sum()\n",
        "# is the dataset balanced\n",
        "train_df['label'].value_counts()"
      ],
      "metadata": {
        "id": "HAVKqFty9BXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CountVectorizer is used to create Bag of Words (BOW)"
      ],
      "metadata": {
        "id": "q7OKgBbQS90D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# find top 20 words in negative reviews\n",
        "train_neg_df = train_df[train_df['label'] == 0]\n",
        "\n",
        "count_vectorizer = CountVectorizer(analyzer='word', stop_words='english', max_df=0.8 ,min_df=100)\n",
        "bag_of_words = count_vectorizer.fit_transform(train_neg_df['text'])\n",
        "sum_words = bag_of_words.sum(axis=0)\n",
        "\n",
        "words_freq = [\n",
        "    (word, sum_words[0, idx]) for word, idx in count_vectorizer.vocabulary_.items()\n",
        "]\n",
        "\n",
        "words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "\n",
        "x, y = zip(*words_freq[:30])\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.bar(x,y)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Top30 Words used in Negative Reviews')\n",
        "#!mkdir assets\n",
        "#plt.savefig('assets/Negative_word.webp', bbox_inches='tight')"
      ],
      "metadata": {
        "id": "myy0f5JR-a_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find top 20 words in positive reviews\n",
        "train_pos_df = train_df[train_df['label'] == 1]\n",
        "\n",
        "count_vectorizer = CountVectorizer(analyzer='word', stop_words='english')\n",
        "bag_of_words = count_vectorizer.fit_transform(train_pos_df['text'])\n",
        "sum_words = bag_of_words.sum(axis=0)\n",
        "\n",
        "words_freq = [\n",
        "    (word, sum_words[0, idx]) for word, idx in count_vectorizer.vocabulary_.items()\n",
        "]\n",
        "\n",
        "words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "\n",
        "x, y = zip(*words_freq[:30])\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.bar(x,y)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Top30 Words used in Positive Reviews')\n",
        "\n",
        "#plt.savefig('assets/Positive_words.webp', bbox_inches='tight')"
      ],
      "metadata": {
        "id": "5jbGBu4H_MrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_rev_train = train_df['text']\n",
        "y_rev_train = train_df['label']\n",
        "\n",
        "X_rev_test = test_df['text']\n",
        "y_rev_test = test_df['label']"
      ],
      "metadata": {
        "id": "hSLDweeogix8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#vect = CountVectorizer().fit(X_rev_train)\n",
        "vect = CountVectorizer(binary=False, max_df=0.80, min_df=10, max_features=10000, stop_words=\"english\").fit(X_rev_train)\n",
        "#with ngram\n",
        "#vect = CountVectorizer(binary=False, max_df=0.80, min_df=10, max_features=10000, stop_words=\"english\", ngram_range=(1,2)).fit(X_rev_train)\n",
        "\n",
        "X_train = vect.transform(X_rev_train)\n",
        "X_test = vect.transform(X_rev_test)\n",
        "print(\"X_train:\\n{}\".format(repr(X_train)))\n"
      ],
      "metadata": {
        "id": "fLWA2TotYCI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0]"
      ],
      "metadata": {
        "id": "rF06Ofz7kTum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Vocabulary:**\n"
      ],
      "metadata": {
        "id": "LwwZwFkHLFQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#print(\"Vocabulary: \", vect.vocabulary_)"
      ],
      "metadata": {
        "id": "5F4AlxjMjQmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original review:\")\n",
        "print(X_rev_train[0])\n",
        "print(\"\\nFirst 20 words that appear in this review (in vocabulary order):\")\n",
        "[print(f\"  {w:15} → {c}\") for w,c in zip(vect.get_feature_names_out()[X_train[0].indices[:20]], X_train[0].data[:20])];"
      ],
      "metadata": {
        "id": "mrIzZ3cOKFq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = vect.get_feature_names_out()\n",
        "print(\"Number of features: {}\".format(len(feature_names)))\n",
        "print(\"First 20 features:\\n{}\".format(feature_names[:20]))\n",
        "print(\"Features 5010 to 5030:\\n{}\".format(feature_names[5010:5030]))\n",
        "print(\"Every 2000th feature:\\n{}\".format(feature_names[::1000]))\n"
      ],
      "metadata": {
        "id": "dU2vd8jUYvpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from warnings import simplefilter\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
        "\n",
        "scores = cross_val_score(LogisticRegression(), X_train, y_rev_train, cv=3)\n",
        "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))"
      ],
      "metadata": {
        "id": "7PxWviEHZIg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CountVectorizer to Extracting Features from Text\n",
        "\n",
        "https://www.geeksforgeeks.org/using-countvectorizer-to-extracting-features-from-text/"
      ],
      "metadata": {
        "id": "tCG-GopXiJ0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb_vec = LogisticRegression()\n",
        "nb_vec.fit(X_train, y_rev_train)\n",
        "preds = nb_vec.predict(X_test)\n",
        "print(classification_report(y_rev_test, preds))"
      ],
      "metadata": {
        "id": "ZDCE10g1Z2ZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Show weights for BOW\n",
        "# https://www.nlplanet.org/course-practical-nlp/01-intro-to-nlp/04-n-grams\n",
        "# create a pandas dataframe that shows the unigrams in each text\n",
        "keys_values_sorted = sorted(list(vect.vocabulary_.items()), key=lambda t: t[1])\n",
        "keys_sorted = list(zip(*keys_values_sorted))[0]\n",
        "\n",
        "# show logistic regression weights\n",
        "from_unigram_to_weight = dict(zip(keys_sorted, nb_vec.coef_[0]))\n",
        "from_unigram_to_weight\n",
        "print('Positive')\n",
        "sorted(zip(nb_vec.coef_[0].round(3) , keys_sorted) , reverse = True)[:20]\n"
      ],
      "metadata": {
        "id": "FCYuBwb21Q1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Negative')\n",
        "sorted(zip(nb_vec.coef_[0].round(2) , keys_sorted) , reverse = False)[:20]"
      ],
      "metadata": {
        "id": "NIu01pX_5zSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TfidfVectorizer**\n",
        "\n",
        "The TfidfVectorizer is a feature extraction technique in the scikit-learn library for converting a collection of raw text documents into a matrix of TF-IDF (Term Frequency-Inverse Document Frequency) features. This is a common step in Natural Language Processing (NLP) and text mining tasks to transform text data into numerical data that machine learning algorithms can work with.\n",
        "\n",
        "How TfidfVectorizer Works\n",
        "Term Frequency (TF): This measures how frequently a term (word) appears in a document. The assumption is that the more frequently a term appears in a document, the more important it is. However, this alone can be misleading, as common words (like \"the\", \"is\", \"and\") will appear frequently in many documents.\n",
        "Inverse Document Frequency (IDF): This measures how important a term is by considering how often it appears across all documents in the dataset. The more documents a term appears in, the less important it is. The IDF value of a term decreases as the number of documents containing the term increases.\n",
        "TF-IDF: The product of TF and IDF. This score gives us an indication of how important a term is within a particular document while reducing the weight of commonly occurring terms that are less informative.\n",
        "Formula of TF-IDF\n",
        "The TF-IDF score for a term t in a document d is calculated as:\n",
        "\n",
        "tf-idf\n",
        "(\n",
        "t\n",
        ",\n",
        "d\n",
        ")\n",
        "=\n",
        "tf\n",
        "(\n",
        "t\n",
        ",\n",
        "d\n",
        ")\n",
        "×\n",
        "idf\n",
        "tf-idf(t,d)=tf(t,d)×idf\n",
        "\n",
        "Where:\n",
        "\n",
        "tf\n",
        "(\n",
        "t\n",
        ",\n",
        "d\n",
        ")\n",
        "tf(t,d)\n",
        " is the term frequency of term t in document d.\n",
        "idf\n",
        "(\n",
        "t\n",
        ")\n",
        "idf(t)\n",
        " is the inverse document frequency of term t, calculated as:\n",
        "idf\n",
        "(\n",
        "t\n",
        ")\n",
        "=\n",
        "log\n",
        "⁡\n",
        "(\n",
        "N\n",
        "1\n",
        "+\n",
        "df\n",
        "(\n",
        "t\n",
        ")\n",
        ")\n",
        "idf(t)=log(\n",
        "1+df(t)\n",
        "N\n",
        "​\n",
        " )\n",
        "\n",
        "Where:\n",
        "\n",
        "N is the total number of documents.\n",
        "df\n",
        "(\n",
        "t\n",
        ")\n",
        "df(t)\n",
        " is the number of documents containing the term t.\n",
        " https://www.geeksforgeeks.org/how-to-store-a-tfidfvectorizer-for-future-use-in-scikit-learn/\n",
        "\n",
        " https://www.linkedin.com/pulse/count-vectorizers-vs-tfidf-natural-language-processing-sheel-saket/\n"
      ],
      "metadata": {
        "id": "6KIH6lTu6FID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_rev_vec = TfidfVectorizer(\n",
        "    binary=False, max_df=0.80, min_df=10,\n",
        "    max_features=10000\n",
        ")\n",
        "\n",
        "X_rev_tfidf_train = tfidf_rev_vec.fit_transform(X_rev_train)\n",
        "X_rev_tfidf_test = tfidf_rev_vec.transform(X_rev_test)"
      ],
      "metadata": {
        "id": "2FE2fznU_zCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print idf values\n",
        "df_idf = pd.DataFrame(tfidf_rev_vec.idf_, index=vect.get_feature_names_out(),columns=[\"idf_weights\"])\n",
        "\n",
        "# sort ascending\n",
        "df_idf.sort_values(by=['idf_weights'])"
      ],
      "metadata": {
        "id": "pqNFpjgA9Vi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_rev = LogisticRegression()\n",
        "nb_rev.fit(X_rev_tfidf_train, y_rev_train)"
      ],
      "metadata": {
        "id": "_65lj2qH_4IX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = nb_rev.predict(X_rev_tfidf_test)\n",
        "print(classification_report(y_rev_test, preds))"
      ],
      "metadata": {
        "id": "6ZUiUZzO_7Rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conf_mtx = confusion_matrix(y_rev_test, preds)\n",
        "conf_mtx_plot = ConfusionMatrixDisplay(\n",
        "     confusion_matrix=conf_mtx\n",
        ")\n",
        "conf_mtx_plot.plot(cmap='plasma')"
      ],
      "metadata": {
        "id": "TvHX23WzAUTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Show weights for BOW\n",
        "# https://www.nlplanet.org/course-practical-nlp/01-intro-to-nlp/04-n-grams\n",
        "# create a pandas dataframe that shows the unigrams in each text\n",
        "keys_values_sorted = sorted(list(tfidf_rev_vec.vocabulary_.items()), key=lambda t: t[1])\n",
        "keys_sorted = list(zip(*keys_values_sorted))[0]\n",
        "\n",
        "# show logistic regression weights\n",
        "from_unigram_to_weight = dict(zip(keys_sorted, nb_rev.coef_[0]))\n",
        "from_unigram_to_weight\n",
        "print('Positive')\n",
        "sorted(zip(nb_rev.coef_[0] , keys_sorted) , reverse = True)[:20]"
      ],
      "metadata": {
        "id": "s2qnehMK-tUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Negative')\n",
        "sorted(zip(nb_rev.coef_[0] , keys_sorted) , reverse = False)[:20]"
      ],
      "metadata": {
        "id": "YXLNqpTr7KZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Let's build a pipline**"
      ],
      "metadata": {
        "id": "1-oUWSo78VlX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import BernoulliNB\n",
        "#('clf', LogisticRegression(solver='lbfgs')),])\n",
        "#('clf', DecisionTreeClassifier(max_depth=35))])\n",
        "\n",
        "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english',max_features=10000,ngram_range=(1,2))),\n",
        "                    ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', LogisticRegression(fit_intercept=True))])"
      ],
      "metadata": {
        "id": "_y9VlMk-nxco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_clf.fit(X_rev_train, y_rev_train)\n",
        "text_clf.score(X_rev_test,y_rev_test)"
      ],
      "metadata": {
        "id": "eJEs5rOenyYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_clf.predict(['false awful we we bla bla bla great great']))\n",
        "print(text_clf.predict_proba(['false awful we we bla bla bla great great']))"
      ],
      "metadata": {
        "id": "zrlwLDRb-EGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Let's add PCA/SVD to our features**"
      ],
      "metadata": {
        "id": "8hhT2z6_WTzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "text_clf = Pipeline([\n",
        "    ('vect', CountVectorizer(stop_words='english', max_features=10000, ngram_range=(1,2))),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('svd', TruncatedSVD(n_components=500, random_state=42)),  # no need to densify!\n",
        "    ('clf', LogisticRegression(fit_intercept=True))\n",
        "])"
      ],
      "metadata": {
        "id": "ILrAVFc1U6D5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_clf.fit(X_rev_train, y_rev_train)\n",
        "text_clf.score(X_rev_test,y_rev_test)"
      ],
      "metadata": {
        "id": "_4Inxz6FWjs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_clf.predict(['false awful we we bla bla bla great great']))\n",
        "print(text_clf.predict_proba(['false awful we we bla bla bla great great']))"
      ],
      "metadata": {
        "id": "Ir3xxWikWl3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Im3x-164F-zW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vocabulary Stemmer and Lemmatization"
      ],
      "metadata": {
        "id": "VLDYQAVx-F-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')\n",
        "from sklearn import metrics\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')\n",
        "# Download required data\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "# Initialize stemmer and stopwords\n",
        "stemmer = PorterStemmer()\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "def stem_tokenizer(text):\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', text.lower())\n",
        "    tokens = text.split()\n",
        "    return [stemmer.stem(token) for token in tokens if token not in stop_words]\n",
        "\n",
        "pipelineSGD = Pipeline([\n",
        "    ('vectorizer', CountVectorizer(\n",
        "        tokenizer=stem_tokenizer,\n",
        "        ngram_range=(1, 2),\n",
        "        lowercase=False  # we already lowercased in tokenizer\n",
        "    )),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', SGDClassifier(loss='hinge', alpha=1e-4, max_iter=100, random_state=42))\n",
        "])\n",
        "\n",
        "pipelineSGD.fit(X_rev_train, y_rev_train)\n",
        "\n",
        "y_predSGD = pipelineSGD.predict(X_rev_test)\n",
        "print(\"Predicted: \", y_predSGD)\n",
        "\n",
        "print(\"Accuracy: \", metrics.accuracy_score(y_rev_test, y_predSGD)*100, \"%\")"
      ],
      "metadata": {
        "id": "JeggZyh3xsXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"DONT WATCH THIS MOVIE. its a complete trash\"\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import re\n",
        "ps = PorterStemmer()\n",
        "CLEAN = re.compile(\"<.*?>\")    # to remove everything between \"<>\"\n",
        "result = re.sub(CLEAN, \" \", text)\n",
        "result = re.sub(\"[^a-zA-Z]\" , \" \" , result)\n",
        "result = result.lower()\n",
        "result = result.split()   # to break sentences into words\n",
        "word = [ps.stem(word) for word in result if word not in stopwords.words(\"english\")]\n",
        "result = \" \".join(word)\n",
        "print(result)\n",
        "print(pipelineSGD.predict([text]))"
      ],
      "metadata": {
        "id": "8Ug5fyx8ZUVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The pipeline examples"
      ],
      "metadata": {
        "id": "kXU7NJWmSBo9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "pipelineSVM = Pipeline([\n",
        "    ('vectorizer', CountVectorizer(stop_words = stop_words, ngram_range = (1,2), max_features = None)),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', LinearSVC(C = 0.5, max_iter = 1000, penalty = 'l2', tol = 0.01))\n",
        "])\n",
        "\n",
        "\n",
        "pipelineSVM.fit(X_rev_train, y_rev_train)\n",
        "\n",
        "y_predSVM = pipelineSVM.predict(X_rev_test)\n",
        "print(\"Predicted: \", y_predSVM)\n",
        "\n",
        "print(\"Accuracy: \", metrics.accuracy_score(y_rev_test, y_predSVM)*100, \"%\")"
      ],
      "metadata": {
        "id": "oMgwja6JzBkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_clf = Pipeline([('vect', CountVectorizer(analyzer='word', token_pattern=r'\\w{2,}',  strip_accents='unicode', stop_words=stop_words, max_features=10000,ngram_range=(1,1))),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                      ('clf', LogisticRegression()),])\n",
        "text_clf.fit(X_rev_train, y_rev_train)\n",
        "text_clf.score(X_rev_test,y_rev_test)"
      ],
      "metadata": {
        "id": "8tMmNCCLUgqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Most common words that were used in positive reviews\n",
        "#https://github.com/Kasra1377/IMDB-sentiment-analysis/blob/master/data-preprocessing.ipynb\n",
        "features = text_clf['vect'].get_feature_names_out()\n",
        "sorted(zip(text_clf['clf'].coef_[0] , features) , reverse = True)[:20]\n"
      ],
      "metadata": {
        "id": "p3ra8MheXWAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Most common words that were used in negative reviews\n",
        "sorted(zip(text_clf['clf'].coef_[0] , features))[:20]"
      ],
      "metadata": {
        "id": "pHTvfsFLY0HL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FuxyXI3DezJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Examining the Model Performance"
      ],
      "metadata": {
        "id": "7qJU50fRZGNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text = \"DONT WATCH THIS MOVIE. its a complete trash\""
      ],
      "metadata": {
        "id": "R_929S_XXktN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate original and stemmed\n",
        "print(text_clf.predict_proba([text]))"
      ],
      "metadata": {
        "id": "6eATI5CBZqpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The best sklearn result**"
      ],
      "metadata": {
        "id": "Z7oiUanHi1np"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://www.kaggle.com/code/adamschroeder/countvectorizer-tfidfvectorizer-predict-comments\n",
        "from sklearn.feature_extraction import text\n",
        "my_stop_words = text.ENGLISH_STOP_WORDS.union([\"book\"])\n",
        "\n",
        "pipe = make_pipeline(TfidfVectorizer(\n",
        "                                    stop_words='english',\n",
        "                                    strip_accents='unicode',\n",
        "                                    token_pattern=r'\\w{1,}', #accept tokens that have 1 or more characters\n",
        "                                    analyzer='word',\n",
        "                                    ngram_range=(1, 2),\n",
        "                                    min_df=100),\n",
        "                     LinearSVC(C = 0.5, max_iter = 1000, penalty = 'l2', tol = 0.01))\n",
        "param_grid = {'tfidfvectorizer__max_features': [None],\n",
        "              'tfidfvectorizer__sublinear_tf': [True],\n",
        "              'tfidfvectorizer__smooth_idf': [True],\n",
        "              'tfidfvectorizer__use_idf': [True],\n",
        "              'tfidfvectorizer__stop_words': [None],\n",
        "              'linearsvc__C': [ 0.1],\n",
        "\n",
        "             }\n",
        "grid = GridSearchCV(pipe, param_grid, cv=3)\n",
        "\n",
        "grid3 = grid.fit(X_rev_train, y_rev_train)\n",
        "\n"
      ],
      "metadata": {
        "id": "gtpZdsRyao66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(grid3.best_params_)\n",
        "print(grid3.best_score_)\n"
      ],
      "metadata": {
        "id": "Oo9C_xX0GMQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid3.score(X_rev_test,y_rev_test)"
      ],
      "metadata": {
        "id": "ejLGGDBJGXrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for the comparition best results:\n",
        "# https://paperswithcode.com/sota/sentiment-analysis-on-imdb"
      ],
      "metadata": {
        "id": "GOuBtP6N4_1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(grid3.predict(['ok recommended its greate 10 of 10 movie']))"
      ],
      "metadata": {
        "id": "gRrWDSGrQx4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=grid3.predict(X_rev_test)"
      ],
      "metadata": {
        "id": "6roOReJ3UVLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy: \", metrics.accuracy_score(y_rev_test, y_pred)*100, \"%\")"
      ],
      "metadata": {
        "id": "TuW8KCB0Ue0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = np.asarray(y_rev_test)\n",
        "misclassified = np.where(y_test != grid3.predict(X_rev_test))\n",
        "len(misclassified[0])"
      ],
      "metadata": {
        "id": "2Mr-UPeDUseQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i=200\n",
        "print('True label=',y_test[misclassified[0][i]], ' predicted =', 1-y_test[misclassified[0][i]] )\n",
        "X_rev_test[misclassified[0][i]]\n"
      ],
      "metadata": {
        "id": "u0iMTBUdQTWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm=confusion_matrix(y_test, grid3.predict(X_rev_test))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=grid3.classes_)\n",
        "disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yNgJAji9QUmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's cluster train part of DB"
      ],
      "metadata": {
        "id": "N0IKFFE-fDHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Very light vectorizer (we just want to catch duplicates/spam)\n",
        "vect = TfidfVectorizer(\n",
        "    ngram_range=(1,3),          # trigrams catch entire spam sentences!\n",
        "    max_features=20000,\n",
        "    min_df=3,\n",
        "    stop_words='english',\n",
        "    lowercase=True\n",
        ")\n",
        "\n",
        "X = vect.fit_transform(X_rev_train)   # X_rev_train = list/array of raw reviews\n",
        "\n",
        "# 2. Cluster into 50–200 clusters (MiniBatchKMeans is fast on 50k docs)\n",
        "n_clusters = 100\n",
        "kmeans = MiniBatchKMeans(n_clusters=n_clusters, random_state=42, batch_size=1000)\n",
        "labels = kmeans.fit_predict(X)\n",
        "\n",
        "# 3. Show the biggest clusters → spam jumps out immediately\n",
        "cluster_sizes = np.bincount(labels)\n",
        "biggest_clusters = np.argsort(cluster_sizes)[::-1]\n",
        "\n",
        "print(\"Top 10 biggest clusters (these are almost always spam):\")\n",
        "for i in biggest_clusters[:10]:\n",
        "    size = cluster_sizes[i]\n",
        "    print(f\"\\nCluster {i} → {size} reviews ({size/len(X_rev_train)*100:.1f}%)\")\n",
        "\n",
        "    # Show 3 example reviews from this cluster\n",
        "    examples_idx = np.where(labels == i)[0][:3]\n",
        "    for j, idx in enumerate(examples_idx, 1):\n",
        "        text = X_rev_train[idx]\n",
        "        print(f\"   {j}. \\\"{text.replace(chr(10), ' ')[:180]}...\\\"\")"
      ],
      "metadata": {
        "id": "FBHJdq8mfJl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_keywords(X_rev_train, labels, cluster, n_keep=3) -> str:\n",
        "    mask = (labels == cluster)\n",
        "    text = \". \".join(X_rev_train.loc[mask].astype(str))\n",
        "    kw_model = KeyBERT()\n",
        "    candidates = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 1),\n",
        "                                      top_n=n_keep*2)\n",
        "    #print(candidates)\n",
        "    keywords_with_scores = candidates  # already a list of tuples\n",
        "\n",
        "    # 4. Greedily select non-overlapping keywords\n",
        "    selected = []\n",
        "    for kw, score in keywords_with_scores:\n",
        "        if any(kw in seen for seen in selected):\n",
        "            continue\n",
        "        if any(seen in kw for seen in selected):\n",
        "            continue\n",
        "        selected.append(kw)\n",
        "        if len(selected) >= n_keep:\n",
        "            break\n",
        "\n",
        "    # 5. Return pretty string\n",
        "    if len(selected) > 1:\n",
        "        return (\", \".join(selected[:-1]) + \" and \" + selected[-1]).capitalize()\n",
        "    elif selected:\n",
        "        return selected[0].capitalize()\n",
        "    else:\n",
        "        return \"No keywords\""
      ],
      "metadata": {
        "id": "HdS7tlX7UZd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_keywords_in_text(text_to_analize, n_keep=3) -> str:\n",
        "\n",
        "    text = \". \".join(text_to_analize.astype(str))\n",
        "    kw_model = KeyBERT()\n",
        "    candidates = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 1),\n",
        "                                      top_n=n_keep*2)\n",
        "    #print(candidates)\n",
        "    keywords_with_scores = candidates  # already a list of tuples\n",
        "\n",
        "    # 4. Greedily select non-overlapping keywords\n",
        "    selected = []\n",
        "    for kw, score in keywords_with_scores:\n",
        "        if any(kw in seen for seen in selected):\n",
        "            continue\n",
        "        if any(seen in kw for seen in selected):\n",
        "            continue\n",
        "        selected.append(kw)\n",
        "        if len(selected) >= n_keep:\n",
        "            break\n",
        "\n",
        "    # 5. Return pretty string\n",
        "    if len(selected) > 1:\n",
        "        return (\", \".join(selected[:-1]) + \" and \" + selected[-1]).capitalize()\n",
        "    elif selected:\n",
        "        return selected[0].capitalize()\n",
        "    else:\n",
        "        return \"No keywords\""
      ],
      "metadata": {
        "id": "DLqd60WXYw8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keywords = filter_keywords(X_rev_train, labels, cluster=27, n_keep=3)\n",
        "print(keywords)"
      ],
      "metadata": {
        "id": "ymqWExIJUiIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Top 40 biggest clusters (these are almost always spam):\")\n",
        "for i in biggest_clusters[:40]:\n",
        "    size = cluster_sizes[i]\n",
        "    print(f\"\\nCluster {i} → {size} reviews ({size/len(X_rev_train)*100:.1f}%) Keywords: \", filter_keywords(X_rev_train, labels, cluster=i, n_keep=3))\n",
        "\n",
        "    # Show 3 example reviews from this cluster\n",
        "    examples_idx = np.where(labels == i)[0][:5]\n",
        "    for j, idx in enumerate(examples_idx, 1):\n",
        "        text = X_rev_train[idx]\n",
        "        print(f\"   {j}. \\\"{text.replace(chr(10), ' ')[:180]}...\\\"\")"
      ],
      "metadata": {
        "id": "8_hLv9ZDgFdj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}